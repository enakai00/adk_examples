{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9320905-016f-4ad9-9402-70a94cde7c1d",
   "metadata": {},
   "source": [
    "# Deploying ADK agent on Agent Engine from source files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbde7f-a443-49e2-83f2-373f5baa9ca8",
   "metadata": {},
   "source": [
    "# Deploying ADK agent on Agent Engine from source files\n",
    "\n",
    "This guide explains how to deploy an AI agent built with the **Google Agent Development Kit (ADK)** to **Agent Engine** (Vertex AI Reasoning Engine) using local source files.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Standard deployment for Vertex AI Agent Engine often relies on \"on-the-fly\" serialization of Python objects, which can fail if the agent contains non-serializable components.\n",
    "\n",
    "**The Solution:** Use **Source-Based Deployment**. By providing the path to your source code and specifying an entry point, Agent Engine instantiates the object directly in the cloud, bypassing local serialization errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Steps\n",
    "\n",
    "1. **Environment Initialization:** Set up the Vertex AI SDK with your Project ID and target Location.\n",
    "2. **Source Preparation:** * Create a local directory (e.g., `sample_agent`) for your logic.\n",
    "* Define `agent.py`, where `AdkApp` and `LlmAgent` are instantiated.\n",
    "* Define `requirements.txt` to include `google-adk` and related libraries.\n",
    "\n",
    "\n",
    "3. **Client Configuration:** Initialize the `vertexai.Client` using `v1beta1` features required for Agent Engine management.\n",
    "4. **Deployment Configuration:** Define a configuration dictionary mapping the local source to the cloud via `source_packages`, `entrypoint_module`, and `entrypoint_object`.\n",
    "5. **Validation:** Implement a client to test the deployed agent via asynchronous streaming queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "#### 1. Avoiding Serialization Errors\n",
    "\n",
    "Using `source_packages` and `entrypoint_module` tells Vertex AI to \"install and run\" your code rather than \"copying a memory snapshot\". This is the recommended path for agents using complex third-party libraries.\n",
    "\n",
    "#### 2. Automatic Session Management\n",
    "\n",
    "When running on Agent Engine, the ADK framework automatically utilizes the `VertexAiSessionService`. You do **not** need to explicitly set or configure this service within your code, as the environment handles state persistence automatically.\n",
    "\n",
    "#### 3. Dynamic `app_name` via Environment Variables\n",
    "\n",
    "The `AdkApp` requires the full Reasoning Engine resource name (e.g., `projects/.../reasoningEngines/...`) to manage sessions.\n",
    "\n",
    "* **Automatic Injection:** Agent Engine automatically sets the `GOOGLE_CLOUD_AGENT_ENGINE_ID` environment variable during deployment.\n",
    "* **Implementation:** Use `os.environ.get('GOOGLE_CLOUD_AGENT_ENGINE_ID')` to dynamically set the `app_name`.\n",
    "\n",
    "#### 4. API Versioning\n",
    "\n",
    "This process utilizes the `v1beta1` API version via `HttpOptions` to access the latest Agent Engine management features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198b9a7d-9e67-4b32-92b5-4e5a31b1eb72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "[PROJECT_ID] = !gcloud config list --format 'value(core.project)'\n",
    "LOCATION = 'us-central1'\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d28a124-ee9f-4e31-b120-ef6bec2a387a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p sample_agent\n",
    "cat <<'EOF' >sample_agent/agent.py\n",
    "import os\n",
    "from vertexai.agent_engines import AdkApp\n",
    "from google.adk.agents import LlmAgent\n",
    "\n",
    "root_agent = LlmAgent(\n",
    "    name='frientdly_agent',\n",
    "    model='gemini-2.5-flash',\n",
    "    instruction='Be friendly.',\n",
    ")\n",
    "\n",
    "# entrypoint_object\n",
    "app = AdkApp(\n",
    "    agent=root_agent,\n",
    "    # app_name should be the full Reasoning Engine resource name.\n",
    "    app_name=os.environ.get('GOOGLE_CLOUD_AGENT_ENGINE_ID', 'default-app-name'),\n",
    ")\n",
    "EOF\n",
    "\n",
    "cat <<'EOF' >sample_agent/requirements.txt\n",
    "google-adk==1.21.0\n",
    "google-cloud-aiplatform==1.132.0\n",
    "google-genai==1.56.0\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0381bca0-e2ae-46e9-abff-0896bac7b3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = 'friendly_agent'\n",
    "\n",
    "client = vertexai.Client(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    http_options=HttpOptions(\n",
    "        api_version='v1beta1',\n",
    "        base_url=f'https://{LOCATION}-aiplatform.googleapis.com/'\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = {\n",
    "        'display_name': DISPLAY_NAME,\n",
    "        'source_packages': ['sample_agent'],\n",
    "        'entrypoint_module': 'sample_agent.agent',\n",
    "        'entrypoint_object': 'app',\n",
    "        'requirements_file': 'sample_agent/requirements.txt',\n",
    "        'class_methods': [\n",
    "            {\n",
    "                'name': 'async_stream_query',\n",
    "                'api_mode': 'async_stream',\n",
    "                'description': 'Stream responses from the agent for a given query.',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'input': {'type': 'string', 'description': 'The user query'},\n",
    "                        'config': {'type': 'object', 'description': 'Optional runtime config'}\n",
    "                    },\n",
    "                    'required': ['input']\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'async_create_session',\n",
    "                'api_mode': 'async',\n",
    "                'description': 'Create a new managed session.'\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "remote_agent = client.agent_engines.create(\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadf92f-ece3-464d-a2fd-dd9644f76dec",
   "metadata": {},
   "source": [
    "**Wait a few minutes here for the deployed agent to be ready, and continue to the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72be7f74-133b-41f8-94f7-45dbd7d706df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chat client to test AdkApp\n",
    "class ChatClient:\n",
    "    def __init__(self, app, user_id='default_user'):\n",
    "        self._app = app\n",
    "        self._user_id = user_id\n",
    "        self._session_id = None\n",
    "\n",
    "    async def async_stream_query(self, message):\n",
    "        if not self._session_id:\n",
    "            session = await self._app.async_create_session(\n",
    "                user_id=self._user_id,\n",
    "            )\n",
    "            self._session_id = getattr(session, 'id', None) or session['id']\n",
    "\n",
    "        result = []\n",
    "        async for event in self._app.async_stream_query(\n",
    "            user_id=self._user_id,\n",
    "            session_id=self._session_id,\n",
    "            message=message,\n",
    "        ):\n",
    "#            print('====')\n",
    "#            print(event)\n",
    "#            print('====')\n",
    "            if ('content' in event and 'parts' in event['content']):\n",
    "                response = '\\n'.join(\n",
    "                    [p['text'] for p in event['content']['parts'] if 'text' in p]\n",
    "                )\n",
    "                if response:\n",
    "                    print(response)\n",
    "                    result.append(response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be4fbc9-7c27-4e20-82e2-072749e850bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing quite well, thank you for asking! ðŸ˜Š\n",
      "\n",
      "As an AI, I don't have feelings in the way humans do, but I'm fully operational and ready to help you with anything you need.\n",
      "\n",
      "How are *you* doing today? And is there anything I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "client = ChatClient(remote_agent)\n",
    "_ = await client.async_stream_query('how are you?')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
