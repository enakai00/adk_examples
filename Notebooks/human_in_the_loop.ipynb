{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee29212-8d0c-44e4-b4d3-9b7441c397f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy, datetime, json, os, pprint, time, uuid\n",
    "\n",
    "import vertexai\n",
    "from typing import Any\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    HttpOptions, GenerateContentConfig,\n",
    "    FunctionResponse, Part, Content, FunctionCall\n",
    ")\n",
    "from google.adk.events import Event, EventActions\n",
    "from google.adk.tools import ToolContext\n",
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.memory.in_memory_memory_service import InMemoryMemoryService\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools import LongRunningFunctionTool\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmResponse, LlmRequest\n",
    "from google.adk.agents import LoopAgent\n",
    "\n",
    "[PROJECT_ID] = !gcloud config list --format 'value(core.project)'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=f'gs://{PROJECT_ID}'\n",
    ")\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d071d762-9c76-4ec7-bdbb-6030637d730c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(system_instruction, contents,\n",
    "                      response_schema, temperature=0.4,\n",
    "                      model='gemini-2.0-flash-001'):\n",
    "    client = genai.Client(vertexai=True,\n",
    "                          project=PROJECT_ID, location=LOCATION,\n",
    "                          http_options=HttpOptions(api_version='v1'))\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            temperature=temperature,\n",
    "            top_p=0.5,\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=response_schema,\n",
    "        )\n",
    "    )\n",
    "    return '\\n'.join(\n",
    "        [p.text for p in response.candidates[0].content.parts if p.text]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec0d547-7353-47dd-8f0e-9d0d16c30b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalApp:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent,\n",
    "        app_name='default_app',\n",
    "        user_id='default_user',\n",
    "        state={},\n",
    "    ):\n",
    "        self._agent = agent\n",
    "        self._app_name = app_name\n",
    "        self._user_id = user_id\n",
    "        self._state = state\n",
    "        self._runner = Runner(\n",
    "            app_name=self._app_name,\n",
    "            agent=self._agent,\n",
    "            artifact_service=InMemoryArtifactService(),\n",
    "            session_service=InMemorySessionService(),\n",
    "            memory_service=InMemoryMemoryService(),\n",
    "        )\n",
    "        self._session = None\n",
    "        \n",
    "    async def stream(self, query):\n",
    "        if not self._session:\n",
    "            self._session = await self._runner.session_service.create_session(\n",
    "                app_name=self._app_name,\n",
    "                user_id=self._user_id,\n",
    "                session_id=uuid.uuid4().hex,\n",
    "                state=self._state,\n",
    "            )\n",
    "        content = Content(role='user', parts=[Part.from_text(text=query)])\n",
    "        async_events = self._runner.run_async(\n",
    "            user_id=self._user_id,\n",
    "            session_id=self._session.id,\n",
    "            new_message=content,\n",
    "        )\n",
    "        result = []\n",
    "        author = None\n",
    "        async for event in async_events:\n",
    "            if DEBUG:\n",
    "                print(event)\n",
    "            if (event.content and event.content.parts):\n",
    "                response = '\\n'.join([p.text for p in event.content.parts if p.text])\n",
    "                if response:\n",
    "                    if author != event.author:\n",
    "                        author = event.author\n",
    "                        print(f'\\n[{author}]')\n",
    "                    print(response)\n",
    "                    result.append(response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3587f020-bbd3-42e7-ad56-de21cd4418cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_transfer_toolcall(\n",
    "    callback_context: CallbackContext,\n",
    "    llm_response: LlmResponse,\n",
    "    condition: str=None\n",
    "):\n",
    "    parts = copy.deepcopy(llm_response.content.parts)\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    \n",
    "    if condition:\n",
    "        [target_id] = [edge['target'] for edge in graph['edges']\n",
    "                       if edge['source'] == current_node and edge['condition'] == condition]\n",
    "    else:\n",
    "        [target_id] = [edge['target'] for edge in graph['edges'] if edge['source'] == current_node]\n",
    "    callback_context.state['current_node'] = target_id\n",
    "    if target_id == '__end__':\n",
    "        target_agent = 'root_agent'\n",
    "    else:\n",
    "        [target_agent] = [node['agent'] for node in graph['nodes'] if node['id'] == target_id]\n",
    "    parts.append(Part(\n",
    "        function_call=FunctionCall(\n",
    "            name='transfer_to_agent', args={'agent_name': target_agent}\n",
    "        )\n",
    "    ))\n",
    "    return LlmResponse(\n",
    "        content=Content(role='model', parts=parts) \n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3c0a15-a18a-4df1-a8a3-0ee557753d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_message_llm_response(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> LlmResponse:\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    [text] = [node['message'] for node in graph['nodes'] if node['id'] == current_node]\n",
    "    return LlmResponse(\n",
    "        content=Content(role='model', parts=[Part(text=text)])\n",
    "    )\n",
    "\n",
    "def message_agent_before_model_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> LlmResponse:\n",
    "    llm_response = get_message_llm_response(callback_context, llm_request)\n",
    "    return add_transfer_toolcall(callback_context, llm_response)\n",
    "\n",
    "message_agent_A = LlmAgent(\n",
    "    name='message_agent_A',\n",
    "    model='gemini-2.0-flash', # not used.\n",
    "    before_model_callback=message_agent_before_model_callback,\n",
    ")\n",
    "\n",
    "message_agent_D = LlmAgent(\n",
    "    name='message_agent_D',\n",
    "    model='gemini-2.0-flash', # not used.\n",
    "    before_model_callback=message_agent_before_model_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c491b6f2-c5bd-47cd-b929-2a9e9bb90268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer_next_agent_after_model_callback(\n",
    "    callback_context: CallbackContext, llm_response: LlmResponse\n",
    ") -> LlmResponse:    \n",
    "    return add_transfer_toolcall(callback_context, llm_response)\n",
    "\n",
    "\n",
    "say_something_funny_agent = LlmAgent(\n",
    "    name='say_something_funny_agent',\n",
    "    model='gemini-2.5-flash',\n",
    "    instruction=\"\"\"\n",
    "    日本語で大喜利風に面白い答えを返す。答えの部分だけ出力。前後のコメントは不要。\n",
    "    \"\"\",\n",
    "    after_model_callback=transfer_next_agent_after_model_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd95afe0-7962-4609-87b6-01c1cbb5b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hitl_agent_before_model_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> LlmResponse:\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    if llm_request.contents[-1].parts[0].text == 'For context:':\n",
    "        return get_message_llm_response(callback_context, llm_request)\n",
    "    \n",
    "    conditions = [edge['condition'] for edge in graph['edges']\n",
    "                  if edge['source'] == current_node and 'condition' in edge.keys()]\n",
    "    conditions.append('unknown')\n",
    "\n",
    "    user_response = llm_request.contents[-1].parts[-1].text\n",
    "    instruction = f'Categorize the user input into {conditions}. If you are not sure, categorize it as unknown.'\n",
    "    llm_response = LlmResponse(content=Content(role='model', parts=[]))\n",
    "\n",
    "    response_schema = {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": conditions\n",
    "    }\n",
    "    result = generate_response(instruction, user_response,\n",
    "                      response_schema, temperature=0.2,\n",
    "                      model='gemini-2.0-flash-001')\n",
    "    condition = result.strip('\"')\n",
    "    if condition == 'unknown':\n",
    "        return get_message_llm_response(callback_context, llm_request)\n",
    "    else:\n",
    "        return add_transfer_toolcall(callback_context, llm_response, condition)\n",
    "    \n",
    "\n",
    "hitl_agent = LlmAgent(\n",
    "    name='hitl_agent',\n",
    "    model='gemini-2.0-flash', # not used.\n",
    "    before_model_callback=hitl_agent_before_model_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9678f0f8-387b-4d1f-ac56-97a140b91966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def root_agent_before_model_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> LlmResponse:\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    if current_node == '__end__':\n",
    "        return LlmResponse(\n",
    "            content=Content(\n",
    "                role='model', parts=[Part(text='')],\n",
    "            )\n",
    "        )  \n",
    "    if current_node != '__start__':\n",
    "        return LlmResponse(\n",
    "            content=Content(\n",
    "                role='model', parts=[Part(text='Something strange has happend!')],\n",
    "            )\n",
    "        )\n",
    "    llm_response = LlmResponse(content=Content(role='model', parts=[]))\n",
    "    return add_transfer_toolcall(callback_context, llm_response)\n",
    "\n",
    "root_agent = LlmAgent(\n",
    "    name='root_agent',\n",
    "    model='gemini-2.0-flash', # not used.\n",
    "    before_model_callback=root_agent_before_model_callback,\n",
    "    sub_agents=[\n",
    "        copy.deepcopy(message_agent_A),\n",
    "        copy.deepcopy(message_agent_D),\n",
    "        copy.deepcopy(say_something_funny_agent),\n",
    "        copy.deepcopy(hitl_agent),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13cd51a7-fa03-4601-90f9-6dbfffbb7ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[message_agent_A]\n",
      "面白いことを言いますね！\n",
      "\n",
      "[say_something_funny_agent]\n",
      "話の途中で「えっとぉ…」とか「そのぉ…」とか挟んでくるAI。\n",
      "\n",
      "[hitl_agent]\n",
      "気に入りましたか（はい／いいえ）？\n"
     ]
    }
   ],
   "source": [
    "graph = {\n",
    "  \"nodes\": [\n",
    "    {\"id\": \"A\", \"agent\": \"message_agent_A\", \"message\": \"面白いことを言いますね！\"},\n",
    "    {\"id\": \"B\", \"agent\": \"say_something_funny_agent\"},\n",
    "    {\"id\": \"C\", \"agent\": \"hitl_agent\", \"message\": \"気に入りましたか（はい／いいえ）？\"},\n",
    "    {\"id\": \"D\", \"agent\": \"message_agent_D\", \"message\": \"気に入ってもらえてよかったです。\"},\n",
    "  ],\n",
    "  \"edges\": [\n",
    "    {\"source\": \"__start__\", \"target\": \"A\"},\n",
    "    {\"source\": \"A\", \"target\": \"B\"},\n",
    "    {\"source\": \"B\", \"target\": \"C\"},\n",
    "    {\"source\": \"C\", \"target\": \"D\", \"condition\": \"approve\"},\n",
    "    {\"source\": \"C\", \"target\": \"B\", \"condition\": \"reject\"},\n",
    "    {\"source\": \"D\", \"target\": \"__end__\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "state = {'graph': graph, 'current_node': '__start__'}\n",
    "\n",
    "client = LocalApp(root_agent, state=state)\n",
    "\n",
    "DEBUG=False\n",
    "query = f'''\n",
    "こんな生成AIは嫌だ！\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d26cbd6-4ecc-4ba8-ae7a-c4a351722d05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[say_something_funny_agent]\n",
      "毎日「今日の献立はこれでいい？」と聞いてきて、食材の写真を送らせるAI。\n",
      "\n",
      "[hitl_agent]\n",
      "気に入りましたか（はい／いいえ）？\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "いいえ\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3f0f792-866f-4fc9-a74c-135b459b9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[hitl_agent]\n",
      "気に入りましたか（はい／いいえ）？\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "もう一息。\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5fc0a7-bd12-49c7-843b-ed570bf94a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[say_something_funny_agent]\n",
      "「生成」ボタンを押すたびに、「本当にこの内容でよろしいですか？後悔しませんか？」と不安そうに聞いてくるAI。\n",
      "\n",
      "[hitl_agent]\n",
      "気に入りましたか（はい／いいえ）？\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "だめ。\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23ed481b-0648-4771-b16f-242922af1a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[message_agent_D]\n",
      "気に入ってもらえてよかったです。\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "はい。\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66160c4e-7802-436e-85b7-fb8bdf0dc307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
