{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a325e96-2e0e-4d6a-b88c-95868a08b84a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example implementation of a LangGraph-style workflow and Human-in-the-loop in ADK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1dad8-8b1c-4e46-b720-bcbc8fee0549",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee29212-8d0c-44e4-b4d3-9b7441c397f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy, os, uuid\n",
    "import vertexai\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    HttpOptions, GenerateContentConfig,\n",
    "    FunctionResponse, Part, Content, FunctionCall\n",
    ")\n",
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.memory.in_memory_memory_service import InMemoryMemoryService\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmResponse, LlmRequest\n",
    "\n",
    "[PROJECT_ID] = !gcloud config list --format 'value(core.project)'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=f'gs://{PROJECT_ID}'\n",
    ")\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d071d762-9c76-4ec7-bdbb-6030637d730c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(system_instruction, contents,\n",
    "                      response_schema, temperature=0.4,\n",
    "                      model='gemini-2.0-flash-001'):\n",
    "    client = genai.Client(vertexai=True,\n",
    "                          project=PROJECT_ID, location=LOCATION,\n",
    "                          http_options=HttpOptions(api_version='v1'))\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            temperature=temperature,\n",
    "            top_p=0.5,\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=response_schema,\n",
    "        )\n",
    "    )\n",
    "    return '\\n'.join(\n",
    "        [p.text for p in response.candidates[0].content.parts if p.text]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec0d547-7353-47dd-8f0e-9d0d16c30b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalApp:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent,\n",
    "        app_name='default_app',\n",
    "        user_id='default_user',\n",
    "        state={},\n",
    "    ):\n",
    "        self._agent = agent\n",
    "        self._app_name = app_name\n",
    "        self._user_id = user_id\n",
    "        self._state = state\n",
    "        self._runner = Runner(\n",
    "            app_name=self._app_name,\n",
    "            agent=self._agent,\n",
    "            artifact_service=InMemoryArtifactService(),\n",
    "            session_service=InMemorySessionService(),\n",
    "            memory_service=InMemoryMemoryService(),\n",
    "        )\n",
    "        self._session = None\n",
    "        \n",
    "    async def stream(self, query):\n",
    "        if not self._session:\n",
    "            self._session = await self._runner.session_service.create_session(\n",
    "                app_name=self._app_name,\n",
    "                user_id=self._user_id,\n",
    "                session_id=uuid.uuid4().hex,\n",
    "                state=self._state,\n",
    "            )\n",
    "        content = Content(role='user', parts=[Part.from_text(text=query)])\n",
    "        async_events = self._runner.run_async(\n",
    "            user_id=self._user_id,\n",
    "            session_id=self._session.id,\n",
    "            new_message=content,\n",
    "        )\n",
    "        result = []\n",
    "        author = None\n",
    "        async for event in async_events:\n",
    "            if DEBUG:\n",
    "                print(event)\n",
    "            if (event.content and event.content.parts):\n",
    "                response = '\\n'.join([p.text for p in event.content.parts if p.text])\n",
    "                if response:\n",
    "                    if author != event.author:\n",
    "                        author = event.author\n",
    "                        print(f'\\n[{author}]')\n",
    "                    print(response)\n",
    "                    result.append(response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328fc2d2-32a8-44a4-b80a-e22e697d1673",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3587f020-bbd3-42e7-ad56-de21cd4418cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_transfer_toolcall(\n",
    "    callback_context: CallbackContext,\n",
    "    llm_response: LlmResponse,\n",
    "    condition: str=None\n",
    "):\n",
    "    parts = copy.deepcopy(llm_response.content.parts)\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    \n",
    "    if condition:\n",
    "        [target_id] = [edge['target'] for edge in graph['edges']\n",
    "                       if edge['source'] == current_node and edge['condition'] == condition]\n",
    "    else:\n",
    "        [target_id] = [edge['target'] for edge in graph['edges']\n",
    "                       if edge['source'] == current_node]\n",
    "    callback_context.state['current_node'] = target_id\n",
    "    if target_id == '__end__':\n",
    "        target_agent = 'root_agent'\n",
    "    else:\n",
    "        [target_agent] = [node['agent'] for node in graph['nodes']\n",
    "                          if node['id'] == target_id]\n",
    "    parts.append(Part(\n",
    "        function_call=FunctionCall(\n",
    "            name='transfer_to_agent', args={'agent_name': target_agent}\n",
    "        )\n",
    "    ))\n",
    "    return LlmResponse(\n",
    "        content=Content(role='model', parts=parts) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c78507e-b5f3-44de-b45a-32dae771196b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _text_reponse(text):\n",
    "    return LlmResponse(\n",
    "        content=Content(role='model', parts=[Part(text=text)])\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_message(callback_context, llm_request):\n",
    "    graph = callback_context.state['graph']\n",
    "    current_node = callback_context.state['current_node']\n",
    "    [text] = [node['message'] for node in graph['nodes']\n",
    "              if node['id'] == current_node]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe352d5a-3586-44f4-bd12-a117b5f15f8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions to define nodes for specific roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef3ac8-a350-4da5-b698-85f6a5b651d3",
   "metadata": {},
   "source": [
    "### Node to execute a specified function and displays the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f08739-d91b-4f49-bc93-d881e8e3740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_node(name, function, description=''):\n",
    "\n",
    "    def lambda_node_before_model_callback(\n",
    "        callback_context: CallbackContext, llm_request: LlmRequest\n",
    "    ) -> LlmResponse:\n",
    "        text = function(callback_context=callback_context, llm_request=llm_request)\n",
    "        llm_response = _text_reponse(text)\n",
    "        return _add_transfer_toolcall(callback_context, llm_response)\n",
    "    \n",
    "    return LlmAgent(\n",
    "        name=name,\n",
    "        model='gemini-2.0-flash', # not used.\n",
    "        description=description,\n",
    "        before_model_callback=lambda_node_before_model_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d15dbc-e188-411c-bf03-c1a03515d57e",
   "metadata": {},
   "source": [
    "### Node to display a specified message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d5e23f-3196-4a5c-a5a7-7bc8611db643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def message_node(name):\n",
    "    return lambda_node(name, _get_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbc489-2ce2-4d2d-9588-469d1f042bf9",
   "metadata": {},
   "source": [
    "### Node to process with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c491b6f2-c5bd-47cd-b929-2a9e9bb90268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_node(name, model, instruction, description='', tools=[]):\n",
    "    def transfer_next_agent_after_model_callback(\n",
    "        callback_context: CallbackContext, llm_response: LlmResponse\n",
    "    ) -> LlmResponse:    \n",
    "        return _add_transfer_toolcall(callback_context, llm_response)\n",
    "\n",
    "    return LlmAgent(\n",
    "        name=name,\n",
    "        model=model,\n",
    "        description=description,\n",
    "        instruction=instruction,\n",
    "        tools=tools,\n",
    "        after_model_callback=transfer_next_agent_after_model_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be0a19-d4a3-46f9-a99d-d4023a10dec3",
   "metadata": {},
   "source": [
    "### Nodes to execute human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd95afe0-7962-4609-87b6-01c1cbb5b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hitl_node(name, description=''):\n",
    "    def _hitl_agent_before_model_callback(\n",
    "        callback_context: CallbackContext, llm_request: LlmRequest\n",
    "    ) -> LlmResponse:\n",
    "        graph = callback_context.state['graph']\n",
    "        current_node = callback_context.state['current_node']\n",
    "        message = _get_message(callback_context, llm_request)\n",
    "\n",
    "        if llm_request.contents[-1].parts[0].text == 'For context:':\n",
    "            return _text_reponse(message)\n",
    "    \n",
    "        conditions = [edge['condition'] for edge in graph['edges']\n",
    "                      if edge['source'] == current_node and 'condition' in edge.keys()]\n",
    "        conditions.append('unknown')\n",
    "\n",
    "        user_response = llm_request.contents[-1].parts[-1].text\n",
    "        instruction = (f'Categorize the user input into {conditions}.'\n",
    "                        'If you are not sure, categorize it as unknown.')\n",
    "        llm_response = LlmResponse(content=Content(role='model', parts=[]))\n",
    "\n",
    "        response_schema = {'type': 'string', 'enum': conditions}\n",
    "        result = generate_response(instruction, user_response,\n",
    "                          response_schema, temperature=0.2,\n",
    "                          model='gemini-2.0-flash-001')\n",
    "        condition = result.strip('\"')\n",
    "        if condition == 'unknown':\n",
    "            return _text_reponse(message)\n",
    "        else:\n",
    "            return _add_transfer_toolcall(callback_context, llm_response, condition)\n",
    "    \n",
    "    return LlmAgent(\n",
    "        name=name,\n",
    "        model='gemini-2.0-flash', # not used.\n",
    "        description=description,\n",
    "        before_model_callback=_hitl_agent_before_model_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd778b-b189-408a-b41f-8aec6e71ac1d",
   "metadata": {},
   "source": [
    "### Root node, the starting point of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95b9785-b1b4-4fa4-9fd3-f5f7725d14aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def root_node(name, sub_agents, description=''):\n",
    "    def _root_agent_before_model_callback(\n",
    "        callback_context: CallbackContext, llm_request: LlmRequest\n",
    "    ) -> LlmResponse:\n",
    "        graph = callback_context.state['graph']\n",
    "        current_node = callback_context.state['current_node']\n",
    "        if current_node == '__end__':\n",
    "            return _text_reponse('')\n",
    "        if current_node != '__start__':\n",
    "            return _text_reponse('Something strange has happend!')\n",
    "        return _add_transfer_toolcall(callback_context, _text_reponse(''))\n",
    "    \n",
    "    return LlmAgent(\n",
    "        name=name,\n",
    "        model='gemini-2.0-flash', # not used\n",
    "        description=description,\n",
    "        sub_agents=sub_agents,\n",
    "        before_model_callback=_root_agent_before_model_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd483f83-176c-4054-b22e-f40babf57d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee03ad2-ce85-46fa-a919-f9e9bb668f00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Node definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1cfde7-c34c-4d97-8e94-2f0cee81e15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "say_something_funny_agent = llm_node(\n",
    "    name='say_something_funny_agent',\n",
    "    model='gemini-2.5-flash',\n",
    "    instruction=\"\"\"\n",
    "    Given a topic, say something funny in a single sentence.\n",
    "    Only output the answer. No comments before or after.\n",
    "    \"\"\",\n",
    "    description=\"An agent to say something funny.\",\n",
    ")\n",
    "\n",
    "root_agent = root_node(\n",
    "    name='root_agent',\n",
    "    sub_agents=[\n",
    "        message_node('start_message'),\n",
    "        message_node('end_message'),\n",
    "        say_something_funny_agent,\n",
    "        hitl_node('htil_node'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58682697-96b9-4181-beda-ed973c3213ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cd51a7-fa03-4601-90f9-6dbfffbb7ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = {\n",
    "  \"nodes\": [\n",
    "    {\"id\": \"A\", \"agent\": \"start_message\", \"message\": \"I wall say something funny!\"},\n",
    "    {\"id\": \"B\", \"agent\": \"say_something_funny_agent\"},\n",
    "    {\"id\": \"C\", \"agent\": \"htil_node\", \"message\": \"Do you like it? (Yes/No)\"},\n",
    "    {\"id\": \"D\", \"agent\": \"end_message\", \"message\": \"I'm glad to hear it.\"},\n",
    "  ],\n",
    "  \"edges\": [\n",
    "    {\"source\": \"__start__\", \"target\": \"A\"},\n",
    "    {\"source\": \"A\", \"target\": \"B\"},\n",
    "    {\"source\": \"B\", \"target\": \"C\"},\n",
    "    {\"source\": \"C\", \"target\": \"D\", \"condition\": \"approve\"},\n",
    "    {\"source\": \"C\", \"target\": \"B\", \"condition\": \"reject\"},\n",
    "    {\"source\": \"D\", \"target\": \"__end__\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "state = {'graph': graph, 'current_node': '__start__'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b011dc9-6e4f-4703-a695-185c7316ec55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c99067-7414-4486-b13a-9bb24920d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[start_message]\n",
      "I wall say something funny!\n",
      "\n",
      "[say_something_funny_agent]\n",
      "AI's intelligence is like a super-smart dog: it can fetch any data you want, but it still can't explain why it chases its own tail.\n",
      "\n",
      "[htil_node]\n",
      "Do you like it? (Yes/No)\n"
     ]
    }
   ],
   "source": [
    "client = LocalApp(root_agent, state=state)\n",
    "DEBUG = False\n",
    "\n",
    "query = '''\n",
    "Say something regarding AI and its intelligence.\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d26cbd6-4ecc-4ba8-ae7a-c4a351722d05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[say_something_funny_agent]\n",
      "AI's intelligence is so advanced, it can simulate a human perfectly, except for one thing: it still can't pretend to enjoy small talk.\n",
      "\n",
      "[htil_node]\n",
      "Do you like it? (Yes/No)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "nope.\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3f0f792-866f-4fc9-a74c-135b459b9527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[htil_node]\n",
      "Do you like it? (Yes/No)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "more effort!\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5fc0a7-bd12-49c7-843b-ed570bf94a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[say_something_funny_agent]\n",
      "AI's intelligence is so vast, it can beat the world's best chess player, but it still can't figure out why humans keep saying \"bless you\" after a sneeze.\n",
      "\n",
      "[htil_node]\n",
      "Do you like it? (Yes/No)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "I mean no.\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ed481b-0648-4771-b16f-242922af1a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[end_message]\n",
      "I'm glad to hear it.\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "ok.\n",
    "'''\n",
    "result = await client.stream(query)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
