{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08708d3-0f11-41a9-a34f-7da6d3a34a8f",
   "metadata": {},
   "source": [
    "# ADK を Workflow tool として無理やり利用するサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0edb6-37a7-4a09-b9cf-33316acfac67",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a29ece-5ca4-41b5-a1e6-dcef27b266c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy, json, os, re, uuid\n",
    "import vertexai\n",
    "from google.genai.types import Part, Content, FunctionCall\n",
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.memory.in_memory_memory_service import InMemoryMemoryService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmResponse, LlmRequest\n",
    "from google.adk.tools import ToolContext\n",
    "\n",
    "[PROJECT_ID] = !gcloud config list --format 'value(core.project)'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d5c35b-45e7-4fe9-9bdf-5bd924d775a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalApp:\n",
    "    def __init__(self, agent, user_id='default_user', state={}):\n",
    "        self._agent = agent\n",
    "        self._user_id = user_id\n",
    "        self._runner = Runner(\n",
    "            app_name=self._agent.name,\n",
    "            agent=self._agent,\n",
    "            artifact_service=InMemoryArtifactService(),\n",
    "            session_service=InMemorySessionService(),\n",
    "            memory_service=InMemoryMemoryService(),\n",
    "        )\n",
    "        self._state = state\n",
    "        self._session = None\n",
    "        \n",
    "    async def stream(self, query):\n",
    "        if not self._session:\n",
    "            self._session = await self._runner.session_service.create_session(\n",
    "                app_name=self._agent.name,\n",
    "                user_id=self._user_id,\n",
    "                session_id=uuid.uuid4().hex,\n",
    "                state=self._state,\n",
    "            )\n",
    "        content = Content(role='user', parts=[Part.from_text(text=query)])\n",
    "        async_events = self._runner.run_async(\n",
    "            user_id=self._user_id,\n",
    "            session_id=self._session.id,\n",
    "            new_message=content,\n",
    "        )\n",
    "        result = []\n",
    "        async for event in async_events:\n",
    "            if DEBUG:\n",
    "                print(f'----\\n{event}\\n----')\n",
    "            if (event.content and event.content.parts):\n",
    "                response = '\\n'.join([p.text for p in event.content.parts if p.text])\n",
    "                if response:\n",
    "                    print(response)\n",
    "                    result.append(response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c045490-977f-4d28-a74c-bb3dd8b60f31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Workflow 制御のコールバック関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17987dd3-8914-4ab4-8373-3e9c9d010d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def workflow_tool_callback(tool_name):\n",
    "    async def before_model_callback(\n",
    "        callback_context: CallbackContext, llm_request: LlmRequest\n",
    "    ) -> LlmResponse:\n",
    "        \n",
    "        def is_transferred(last_text, function_name):\n",
    "            #text=\"[root_agent] `transfer_to_agent` tool returned result: {'result': None}\")]\n",
    "            transfer_message = ' `transfer_to_agent` tool returned result: '\n",
    "            if transfer_message in last_text or function_name == 'transfer_to_agent':\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        agent_name = callback_context.agent_name\n",
    "        function_name, last_text = None, ''\n",
    "        if llm_request.contents and llm_request.contents[-1].parts[-1]:\n",
    "            last_part = llm_request.contents[-1].parts[-1]\n",
    "            if last_part.function_response:\n",
    "                function_name = last_part.function_response.name\n",
    "            if last_part.text:\n",
    "                last_text = last_part.text\n",
    "            \n",
    "        # Finish when transferred to root_agent.\n",
    "        if callback_context.agent_name == 'root_agent':\n",
    "            if is_transferred(last_text, function_name):\n",
    "                return LlmResponse(\n",
    "                    content=Content(role='model', parts=[Part(text='done')]) \n",
    "                )\n",
    "\n",
    "        # Run tool when directly called or transferred to me.\n",
    "        if (not llm_request.contents) or is_transferred(last_text, function_name):\n",
    "            part = Part(function_call=FunctionCall(name=tool_name, args={}))\n",
    "            return LlmResponse(\n",
    "                content=Content(role='model', parts=[part]) \n",
    "            )\n",
    "\n",
    "        # Transfer to next_agent\n",
    "        response = llm_request.contents[-1].parts[-1].function_response.response\n",
    "        next_agent = agent_name # Default to myself\n",
    "        if 'next_agent' in response:\n",
    "            next_agent = response['next_agent']\n",
    "        part = Part(function_call=FunctionCall(\n",
    "                        name='transfer_to_agent',\n",
    "                        args={'agent_name': next_agent}))\n",
    "        return LlmResponse(\n",
    "            content=Content(role='model', parts=[part]) \n",
    "        )\n",
    "    \n",
    "    return before_model_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77457307-4368-44a8-a0a5-c62f30d907ef",
   "metadata": {},
   "source": [
    "## サンプル実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dc216-a174-4cc7-9f34-f6caa362bbcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "`ping_agent` は `ping_tool()` を実行して、`pong_agent` に遷移する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ed6841-5a6b-4d06-80d5-61e373d45277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ping_tool(tool_context: ToolContext) -> dict:\n",
    "    count = tool_context.state.get('count')\n",
    "    count -= 1\n",
    "    tool_context.state['count'] = count\n",
    "\n",
    "    print('ping', count)\n",
    "    return {'next_agent': 'pong_agent'}\n",
    "\n",
    "\n",
    "ping_agent = LlmAgent(\n",
    "    model='gemini-2.0-flash-001', # not used\n",
    "    name='ping_agent',\n",
    "    description='An agent that always run ping_tool.',\n",
    "    instruction='',\n",
    "    tools=[ping_tool],\n",
    "    before_model_callback = workflow_tool_callback('ping_tool'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08310468-566c-4b56-be76-daf221a1d06b",
   "metadata": {
    "tags": []
   },
   "source": [
    "`pong_agent` は `pong_tool()` を実行して、`ping_agent` に遷移する。\n",
    "\n",
    "ただし、`count <= 0` で `root_agent` に遷移する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3af5423-318c-4c34-b1e9-c2bee77e455c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pong_tool(tool_context: ToolContext) -> dict:\n",
    "    count = tool_context.state.get('count')\n",
    "    count -= 1\n",
    "    tool_context.state['count'] = count\n",
    "\n",
    "    print('pong', count)\n",
    "    if count <= 0:\n",
    "        return {'next_agent': 'root_agent'}\n",
    "    return {'next_agent': 'ping_agent'}\n",
    "\n",
    "\n",
    "pong_agent = LlmAgent(\n",
    "    model='gemini-2.0-flash-001', # not used\n",
    "    name='pong_agent',\n",
    "    description='An agent that always run pong_tool.',\n",
    "    instruction='',\n",
    "    tools=[pong_tool],\n",
    "    before_model_callback = workflow_tool_callback('pong_tool'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8e61a-7cef-41df-87f4-a7856224a667",
   "metadata": {},
   "source": [
    "`root_agent` は初回呼び出し時に `ping_agent` に遷移する。\n",
    "\n",
    "他のエージェントから遷移してきた場合は、そこで終了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1d6827c-95f5-4daa-a64c-a3cbc7386cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def root_tool(tool_context: ToolContext) -> dict:\n",
    "    return {'next_agent': 'ping_agent'}\n",
    "\n",
    "root_agent = LlmAgent(\n",
    "    model='gemini-2.0-flash-001', # not used\n",
    "    name='root_agent',\n",
    "    description='An agent that always run root_tool.',\n",
    "    instruction='',\n",
    "    sub_agents = [\n",
    "        copy.deepcopy(ping_agent),\n",
    "        copy.deepcopy(pong_agent),\n",
    "    ],\n",
    "    tools = [root_tool],\n",
    "    before_model_callback = workflow_tool_callback('root_tool'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27c957-1c8f-4fff-98ab-34b1665991e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "`root_agent` -> `ping_agetn` -> `pong_agent` -> ... -> `root_agent` のワークフローが実行される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e065961c-0800-4611-95e2-a2c81a33d9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping 4\n",
      "pong 3\n",
      "ping 2\n",
      "pong 1\n",
      "ping 0\n",
      "pong -1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "state = {'count': 5}\n",
    "client = LocalApp(root_agent, state=state)\n",
    "\n",
    "DEBUG = False\n",
    "_ = await client.stream('')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
