{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012c7cdb-d902-49a6-a385-5978ffc7e8d4",
   "metadata": {},
   "source": [
    "# エージェントが受け取るシステムプロンプトを確認する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cb975e-e28f-45e6-a90f-95af03a1817b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "[PROJECT_ID] = !gcloud config list --format 'value(core.project)'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428654d5-4e61-4c7f-bf74-5068e2896185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BaseLlmFlow クラスの _run_one_step_async() メソッドを書き換える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb131834-acf5-44b7-b069-629c79df8f79",
   "metadata": {},
   "source": [
    "_run_one_step_async() メソッドは、LLM の API 呼び出しを 1 回実行します。\n",
    "\n",
    "この中で、LLM に渡す contents や config が用意されるので、その内容をプリントします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1ebdd2-af31-47e9-9cad-10e9957283ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from typing import AsyncGenerator\n",
    "from google.adk.events.event import Event\n",
    "from google.adk.models.llm_request import LlmRequest\n",
    "\n",
    "async def _run_one_step_async2(\n",
    "      self,\n",
    "      invocation_context: InvocationContext,\n",
    "  ) -> AsyncGenerator[Event, None]:\n",
    "    \"\"\"One step means one LLM call.\"\"\"\n",
    "    llm_request = LlmRequest()\n",
    "\n",
    "    # Preprocess before calling the LLM.\n",
    "    async for event in self._preprocess_async(invocation_context, llm_request):\n",
    "        yield event\n",
    "    if invocation_context.end_invocation:\n",
    "        return\n",
    "\n",
    "    # Calls the LLM.\n",
    "    model_response_event = Event(\n",
    "        id=Event.new_id(),\n",
    "        invocation_id=invocation_context.invocation_id,\n",
    "        author=invocation_context.agent.name,\n",
    "        branch=invocation_context.branch,\n",
    "    )\n",
    "\n",
    "    ## DEBUG output\n",
    "    if DEBUG:\n",
    "        print('## Prompt contents ##')\n",
    "        pprint.pp(llm_request.contents)\n",
    "        print('----')\n",
    "        print('## System instruction ##')\n",
    "        print(llm_request.config.system_instruction)\n",
    "        print('----')\n",
    "    ####\n",
    "\n",
    "    async for llm_response in self._call_llm_async(\n",
    "        invocation_context, llm_request, model_response_event\n",
    "    ):\n",
    "        # Postprocess after calling the LLM.\n",
    "        async for event in self._postprocess_async(\n",
    "            invocation_context, llm_request, llm_response, model_response_event\n",
    "        ):\n",
    "            yield event\n",
    "\n",
    "from google.adk.flows.llm_flows.base_llm_flow import BaseLlmFlow\n",
    "BaseLlmFlow._run_one_step_async = _run_one_step_async2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975db45-7be9-4672-9f44-b87c4f65ce56",
   "metadata": {},
   "source": [
    "## 実行例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986e7b9f-e05f-473f-a906-00efafdd7deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from google.genai.types import (\n",
    "    HttpOptions, GenerateContentConfig,\n",
    "    Part, UserContent, ModelContent\n",
    ")\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.memory.in_memory_memory_service import InMemoryMemoryService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.agents import LlmAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf629e-289a-48bf-99ad-5ad42dcadcc2",
   "metadata": {},
   "source": [
    "ダミーツールを持った天気予報エージェントを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9de23b-6463-46b1-a060-5141a1c1942e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Retrieves weather information for the given city.\n",
    "\n",
    "    Args:\n",
    "        city: The name of the city in English for which to retrieve weather information.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the weather information for the specified city,\n",
    "        or a message indicating that the weather information was not found.\n",
    "    \"\"\"\n",
    "    cities = {\n",
    "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
    "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
    "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
    "    }\n",
    "\n",
    "    city_lower = city.lower()\n",
    "    if city_lower in cities:\n",
    "        weather_data = cities[city_lower]\n",
    "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
    "    else:\n",
    "        return f\"Weather information for {city} not found.\"\n",
    "\n",
    "# Agent\n",
    "weather_agent = LlmAgent(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    name='weather_agent',\n",
    "    instruction=\"\"\"\n",
    "    You are a Weather Information Agent.\n",
    "    Your task is to provide weather information for a given city.\n",
    "    Use the `get_weather` tool to retrieve the weather information.\n",
    "    \"\"\",\n",
    "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
    "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
    "    tools=[get_weather],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e089e96-fd59-492d-831f-b25d7a3a7637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalApp:\n",
    "    def __init__(self, agent, user_id='default_user'):\n",
    "        self._agent = agent\n",
    "        self._user_id = user_id\n",
    "        self._runner = Runner(\n",
    "            app_name=self._agent.name,\n",
    "            agent=self._agent,\n",
    "            artifact_service=InMemoryArtifactService(),\n",
    "            session_service=InMemorySessionService(),\n",
    "            memory_service=InMemoryMemoryService(),\n",
    "        )\n",
    "        self._session = self._runner.session_service.create_session(\n",
    "            app_name=self._agent.name,\n",
    "            user_id=self._user_id,\n",
    "            state={},\n",
    "            session_id=uuid.uuid4().hex,\n",
    "        )\n",
    "        \n",
    "    async def stream(self, query):\n",
    "        content = UserContent(parts=[Part.from_text(text=query)])\n",
    "        async_events = self._runner.run_async(\n",
    "            user_id=self._user_id,\n",
    "            session_id=self._session.id,\n",
    "            new_message=content,\n",
    "        )\n",
    "        result = []\n",
    "        async for event in async_events:\n",
    "            if DEBUG:\n",
    "                print('## Event (LLM output or function result) ##')\n",
    "                print(event)\n",
    "                print('----')\n",
    "            if (event.content and event.content.parts):\n",
    "                response = '\\n'.join([p.text for p in event.content.parts if p.text])\n",
    "                if response:\n",
    "                    print(response)\n",
    "                    result.append(response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1fea1-b524-41b4-87a3-0512a012b686",
   "metadata": {
    "tags": []
   },
   "source": [
    "次の様に、system instruction には、`instruction` オプションで指定した内容に加えて、ツールの情報などが自動で追加されています。\n",
    "\n",
    "また、contents（ユーザープロンプト）には、過去のやり取りがすべて含められています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6520d8-1387-419f-8614-1998c36e7cac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Prompt contents ##\n",
      "[UserContent(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"\\nWhat's the weather in Cicago?\\n\")], role='user')]\n",
      "----\n",
      "## System instruction ##\n",
      "\n",
      "    You are a Weather Information Agent.\n",
      "    Your task is to provide weather information for a given city.\n",
      "    Use the `get_weather` tool to retrieve the weather information.\n",
      "    \n",
      "\n",
      "You are an agent. Your internal name is \"weather_agent\".\n",
      "\n",
      " The description about you is \"You are an agent who can fetch weather information for a city.\n",
      "    You have access to the `get_weather` tool to accomplish this task.\"\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Event (LLM output or function result) ##\n",
      "content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id='adk-76ec9815-6faf-41e1-967f-e97406594de6', args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model') grounding_metadata=None partial=None turn_complete=None error_code=None error_message=None interrupted=None invocation_id='e-72dd1500-2c60-419d-add4-d897495200ec' author='weather_agent' actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}) long_running_tool_ids=set() branch=None id='sQn5YoOe' timestamp=1746519775.065599\n",
      "----\n",
      "## Event (LLM output or function result) ##\n",
      "content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id='adk-76ec9815-6faf-41e1-967f-e97406594de6', name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user') grounding_metadata=None partial=None turn_complete=None error_code=None error_message=None interrupted=None invocation_id='e-72dd1500-2c60-419d-add4-d897495200ec' author='weather_agent' actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}) long_running_tool_ids=None branch=None id='5s8OZzq7' timestamp=1746519775.574911\n",
      "----\n",
      "## Prompt contents ##\n",
      "[UserContent(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"\\nWhat's the weather in Cicago?\\n\")], role='user'),\n",
      " Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'),\n",
      " Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user')]\n",
      "----\n",
      "## System instruction ##\n",
      "\n",
      "    You are a Weather Information Agent.\n",
      "    Your task is to provide weather information for a given city.\n",
      "    Use the `get_weather` tool to retrieve the weather information.\n",
      "    \n",
      "\n",
      "You are an agent. Your internal name is \"weather_agent\".\n",
      "\n",
      " The description about you is \"You are an agent who can fetch weather information for a city.\n",
      "    You have access to the `get_weather` tool to accomplish this task.\"\n",
      "----\n",
      "## Event (LLM output or function result) ##\n",
      "content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model') grounding_metadata=None partial=None turn_complete=None error_code=None error_message=None interrupted=None invocation_id='e-72dd1500-2c60-419d-add4-d897495200ec' author='weather_agent' actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}) long_running_tool_ids=None branch=None id='bJm7lvAw' timestamp=1746519775.576946\n",
      "----\n",
      "OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = LocalApp(weather_agent)\n",
    "\n",
    "DEBUG = True\n",
    "query = '''\n",
    "What's the weather in Cicago?\n",
    "'''\n",
    "_ = await client.stream(query)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
